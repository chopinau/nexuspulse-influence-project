# -*- coding: utf-8 -*-
"""
Report Engine - Single File Soldier Version
Phase 2: Structured JSON Output & Analysis
ç‹¬ç«‹è¿è¡Œç‰ˆæœ¬ï¼Œä¸ä¾èµ– BettaFish çš„ä»»ä½•å†…éƒ¨ç»„ä»¶ã€‚
ç›´æ¥è°ƒç”¨ DeepSeek API ç”Ÿæˆç»“æ„åŒ–æƒ…æŠ¥å¹¶å­˜å…¥ Supabaseã€‚
"""

import os
import sys
import json
import re
import time
import random
import requests
import argparse
import logging
from datetime import datetime

# ================= Configuration =================

# é»˜è®¤é…ç½®
DEFAULT_API_BASE = "https://api.deepseek.com"
DEFAULT_MODEL = "deepseek-chat"

# Topic Matrix for Random Selection (Fallback)
TOPIC_MATRIX = [
    "Artificial Intelligence Market Trends",
    "Global Semiconductor Industry",
    "Electric Vehicle Market Dynamics",
    "Cryptocurrency & Blockchain Updates",
    "Renewable Energy Transition",
    "Biotechnology & Genomics",
    "Space Exploration & Commercialization",
    "Cybersecurity Threats & Solutions"
]

# ================= Logger =================

class SimpleLogger:
    def info(self, msg):
        print(f"[INFO] {datetime.now().strftime('%H:%M:%S')} - {msg}")
    
    def success(self, msg):
        print(f"[SUCCESS] {datetime.now().strftime('%H:%M:%S')} - {msg}")
    
    def error(self, msg):
        print(f"[ERROR] {datetime.now().strftime('%H:%M:%S')} - {msg}")
    
    def warning(self, msg):
        print(f"[WARN] {datetime.now().strftime('%H:%M:%S')} - {msg}")

logger = SimpleLogger()

# ================= Core Functions =================

def search_and_extract(topic):
    """
    Simulated search and extraction.
    In a real-world scenario, this would use a search API (Google/Bing) and a scraper.
    """
    logger.info(f"ğŸ” æ­£åœ¨æœç´¢å…³äº '{topic}' çš„æœ€æ–°ä¿¡æ¯...")
    
    # Placeholder for scraping logic. 
    # Since we want to rely on the AI's internal knowledge if scraping is not set up:
    return f"Market intelligence data regarding {topic}. (Source: Internal Knowledge & Analysis)"

def analyze_with_deepseek(topic, context, api_key, base_url, model=DEFAULT_MODEL):
    """
    Phase 2: Analyze topic with DeepSeek and return structured JSON.
    """
    logger.info(f"ğŸ§  æ­£åœ¨è°ƒç”¨ AI åˆ†æ: {topic}...")
    
    # System Prompt: å¼ºåˆ¶ JSON æ ¼å¼è¾“å‡ºï¼Œå®šä¹‰é‡åŒ–æŒ‡æ ‡ 
    system_prompt = """You are NexusPulse, an advanced AI Market Intelligence Analyst. 
Your goal is to convert raw scraped data into actionable financial intelligence. 

You MUST reply with a valid JSON object ONLY. No markdown formatting outside the JSON. 
The JSON structure must be: 
{ 
    "title": "A punchy, investor-focused title", 
    "summary": "A concise executive summary (max 200 words)", 
    "sentiment_score": 0 to 100 (0=Bearish, 50=Neutral, 100=Bullish), 
    "confidence_index": 0 to 10 (Based on data quality and source credibility), 
    "key_entities": ["Company A", "Token B", "Person C"], 
    "actionable_insight": "One specific strategic recommendation for investors/exporters.", 
    "risk_alert": "Potential downside or risk factor." 
} 
""" 

    user_prompt = f""" 
Analyze the following scraped news regarding: "{topic}". 
Ignore irrelevant ads or navigation text. Focus on causal chains (Effect -> Impact). 

=== RAW DATA START === 
{context} 
=== RAW DATA END === 
""" 

    headers = { 
        "Authorization": f"Bearer {api_key}", 
        "Content-Type": "application/json" 
    } 

    payload = { 
        "model": model, 
        "messages": [ 
            {"role": "system", "content": system_prompt}, 
            {"role": "user", "content": user_prompt} 
        ], 
        "temperature": 0.4, # é™ä½æ¸©åº¦ä»¥ä¿è¯ JSON æ ¼å¼ç¨³å®š 
        "response_format": {"type": "json_object"} # å¦‚æœ API æ”¯æŒ (DeepSeek beta å¯èƒ½æ”¯æŒ) 
    } 

    try: 
        url = f"{base_url.rstrip('/')}/chat/completions" 
        response = requests.post(url, headers=headers, json=payload, timeout=120) 
        
        if response.status_code != 200: 
            logger.error(f"AI Error: {response.text}") 
            return None 

        content = response.json()['choices'][0]['message']['content'] 
        
        # æ¸…æ´—å¯èƒ½å­˜åœ¨çš„ Markdown ä»£ç å—æ ‡è®° 
        content = re.sub(r'^```json\s*', '', content) 
        content = re.sub(r'^```\s*', '', content) 
        content = re.sub(r'\s*```$', '', content) 
        
        return json.loads(content) 

    except json.JSONDecodeError: 
        logger.error("AI è¿”å›äº†é JSON æ ¼å¼æ•°æ®ï¼Œè§£æå¤±è´¥ã€‚") 
        logger.error(f"Raw Output: {content[:100]}...") 
        return None 
    except Exception as e: 
        logger.error(f"AI æ¥å£å¼‚å¸¸: {e}") 
        return None 

def save_to_supabase(data, supabase_url, supabase_key): 
    """ 
    ä¿å­˜ç»“æ„åŒ–æ•°æ®ã€‚ 
    ä¸ºäº†å…¼å®¹ç°æœ‰çš„ 'market_news' è¡¨ç»“æ„ (å‡è®¾åªæœ‰ title/content å­—æ®µ)ï¼Œ 
    æˆ‘ä»¬å°† JSON æ‰å¹³åŒ–ä¸º Markdown æ ¼å¼å­˜å…¥ 'content'ï¼ŒåŒæ—¶å°† raw_json å­˜å…¥ metadata (å¦‚æœæœ‰)ã€‚ 
    """ 
    logger.info("ğŸ’¾ æ­£åœ¨å†™å…¥æ•°æ®åº“...")

    # å°† JSON è½¬æ¢ä¸ºäººç±»å¯è¯»çš„ Markdown æŠ¥å‘Š 
    md_content = f""" 
> **Sentiment Score:** {data['sentiment_score']}/100 ğŸ“ˆ | **Confidence:** {data['confidence_index']}/10 ğŸ›¡ï¸ 

### ğŸš€ Actionable Insight 
**{data['actionable_insight']}** 

### ğŸ“Š Executive Summary 
{data['summary']} 

### âš ï¸ Risk Alert 
{data['risk_alert']} 

--- 
*Entities: {', '.join(data['key_entities'])}* 
""" 

    payload = { 
        "title": f"[{data['sentiment_score']}] {data['title']}", # åœ¨æ ‡é¢˜ä¸­ç›´æ¥æ˜¾ç¤ºåˆ†æ•° 
        "content": md_content, 
        "created_at": datetime.now().isoformat(), 
        # å¦‚æœä½ çš„è¡¨æœ‰ 'metadata' JSONB å­—æ®µï¼Œå–æ¶ˆä¸‹é¢æ³¨é‡Š: 
        # "metadata": data 
    } 

    headers = { 
        "apikey": supabase_key, 
        "Authorization": f"Bearer {supabase_key}", 
        "Content-Type": "application/json", 
        "Prefer": "return=minimal" 
    } 

    try: 
        api_url = f"{supabase_url.rstrip('/')}/rest/v1/market_news" 
        response = requests.post(api_url, headers=headers, json=payload, timeout=30) 
        
        if response.status_code in [200, 201]: 
            logger.success("æ•°æ®ä¿å­˜æˆåŠŸï¼") 
            return True 
        else: 
            logger.error(f"Supabase Error: {response.status_code} - {response.text}") 
            return False 
    except Exception as e: 
        logger.error(f"Database Exception: {e}") 
        return False 

# ================= Main Execution ================= 

def main(): 
    # 0. ç¯å¢ƒåˆå§‹åŒ– 
    try: 
        from dotenv import load_dotenv 
        load_dotenv() 
    except: 
        pass 
    
    # å¢åŠ  argparse é€»è¾‘ä»¥æ”¯æŒ GitHub Actions çš„ --query å‚æ•°
    parser = argparse.ArgumentParser(description="NexusPulse Report Engine")
    parser.add_argument("--query", type=str, help="Specify topic manually")
    parser.add_argument("--auto", action="store_true", help="Auto mode")
    args = parser.parse_args()

    api_key = os.environ.get("REPORT_ENGINE_API_KEY") 
    base_url = os.environ.get("REPORT_ENGINE_BASE_URL", DEFAULT_API_BASE) 
    supabase_url = os.environ.get("SUPABASE_URL") or os.environ.get("NEXT_PUBLIC_SUPABASE_URL") 
    supabase_key = os.environ.get("SUPABASE_KEY") or os.environ.get("NEXT_PUBLIC_SUPABASE_ANON_KEY") 

    if not api_key or not supabase_url: 
        logger.error("é…ç½®ç¼ºå¤±: è¯·æ£€æŸ¥ç¯å¢ƒå˜é‡ API_KEY å’Œ SUPABASE_URL") 
        sys.exit(1) 

    # 1. é€‰æ‹©ä¸»é¢˜
    if args.query:
        target_topic = args.query
        logger.info(f"ğŸ¯ æŒ‡å®šä»»åŠ¡ç›®æ ‡: {target_topic}")
    else:
        # éšæœºé€‰æ‹©ä¸€ä¸ªä»¥ä¿æŒ Cron ä»»åŠ¡è½»é‡åŒ–
        target_topic = random.choice(TOPIC_MATRIX) 
        logger.info(f"ğŸ¯ éšæœºä»»åŠ¡ç›®æ ‡: {target_topic}") 

    # 2. æœç´¢ & æå– (The Eyes) 
    context = search_and_extract(target_topic) 
    if not context: 
        logger.warning("ä¿¡æ¯æ”¶é›†ä¸ºç©ºï¼Œè·³è¿‡æœ¬æ¬¡ä»»åŠ¡ã€‚") 
        return 

    # 3. åˆ†æ & é‡åŒ– (The Brain) 
    intelligence = analyze_with_deepseek(target_topic, context, api_key, base_url, DEFAULT_MODEL) 
    
    # 4. å­˜å‚¨ (The Memory) 
    if intelligence: 
        save_to_supabase(intelligence, supabase_url, supabase_key) 
    else: 
        logger.error("æƒ…æŠ¥ç”Ÿæˆå¤±è´¥ã€‚") 

if __name__ == "__main__": 
    main()
